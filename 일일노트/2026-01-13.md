- https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B8_%ED%8A%9C%EB%8B%9D
-  [[Pasted image 20260113094638.png]]
- y=wx+b (weight)
- 특정기능이 제거된 상태에서 최소값. 아무것도 학습되지 않은 상태에서 기본적으로 갖는 값 

- 파인튜닝의 종류(FT)
- `PEFT` : 일부 파라미터만 선택적으로 미세조정하는것(지도 학습일 수도 있고 비지도 일 수도 있다) <-> 풀 파인튜닝
- https://usingsystem.tistory.com/563
- `SFT(Supervised Fine-Tuning)` : 지도 학습(데이터셋에 라벨이 있다)을 기반으로 한 파인튜닝의 기법 - 답이 있음.
- PEFT랑 SFT -> 상호 보완.
- 상호 배타적 관계에서 요구 사항 (xor : 하나를 취하면 하나는 반드시 취할 수 없다)
- `RLHF(Reinforcement Learning from Human Feedback)` : RLHF = 사람 피드백 + 강화학습 (RL)
- Reinforcement : 보강, 강화 
- 
- 강화 학습(RL) : 에이전트가 환경(루프 - 렌덤트리)과 상호작용하면서 보상(reward : 정확도)을 최대화하는 행동 정책(policy : 인간이 피드백)을 학습하는 방법입니다.
- 행동 결과 보상
- 행동 : tool(기능) , invoke 
- 기능을 의도에따라 촘촘하게 정해주는것.
-  
- 함수는 불변객체다. 라는 말은 함수가 외부 상태를 변경하지 않고 항상 동일한 입력에 대해 동일한 출력을 반환하는 순수 함수를 지향하며, 이를 불변성(immutal)을 통해 구현되어 코드의 안정성과 예측 가능성. 
- 모델이랑 툴, 
- print 함수 
- .invoke() 외부 데이터값에 따라 바뀌는 것 : 불변함수가 아니다.
- 함수는 불변 객체다 -> 미듬 넣으면 미듬 나오는거.
- 객체를 가지고 있는 함수는 메소드

- 액션 : 깃허브가 배포라는 목적을 위해서 내부 정책.
- 정책 : 
-  RL 정책 vs I am 정책
: 정책은 객체다(메뉴얼, 함수는 여러개 가지고 있음 상태에 따라서 함수:(액션)가 매핑되있음), 정책은 전략패턴이다 
: 액션도 함수
: 강화한다(웨이트와 강화되는 지점에 따라 액션이 지정되어야 한다)

어쩔땐 뭐하고 어쩔땐 뭐하고 이걸 스스로 하는 거고(내부를 못봄), i am은 현재 권한만 가지고 있다.(내부 볼 수 있음)
상태에 따른 액션을 가지고 있는 객체(safe + action)
Reward(HF)

RL 액션 vs 깃허브 액션
에이전트가 고침(내부를 못 봄) - 미세조정 vs 개발자가 고침(내부를 볼 수 있음)

- SFT + PEFT
-  
- JSONL : JSON + list
260113 @app/service/convert_to_jsonl.py 여기에 , data 밑에 있는 csv 파일의 포맷을 jsonl 로 변경하는 코드를 작성하고 , 하단부에 __main__ 을 추가해줘. 변경된 파일은 data 밑에 확장자만 바뀌어서 들어가도록 해줘.
- DPO(Direct Preference Optimization) : 선호(preference) 데이터만으로 정책을 직접 최적화하는 방법
- 
- 강화학습에서 규칙기반과 정책기반의 차이점
- 규칙기반 : 사람(액션:함수 을 인간이 정하면)
- 정책기반 : 모델(액션:행동 을 에이전트가 정하면)
- ![[Pasted image 20260113144205.png]]
- ![[Pasted image 20260113144643.png]]
- **PPO는 환경 보상(reward)을 최대화하는 강화학습 알고리즘**이고,   RLHF
**DPO는 인간 선호(preference) 데이터를 이용해 보상모델 없이 정책을 직접 정렬하는 방법**입니다.
- RLHF 자체를 Dataset에 집어 넣는 거. 
- raw 데이터 자체를 DPO로 바꾸는 것.
- 인풋이랑 아웃풋이 하나의 쌍이 됨, 선호하는 데이터를 쌍으로 표시하는 것이 DPO

- SFT
- DPO : 답 O
- PPO : 답 X

- RLHF
- SFT
- RLHF(PPO : 지속 배포) RL(DPO)