- 퍼셉트론 : 단층 -> Atom -> 1957
- ANN(Artificial Neural 여러개 니까 Network)  : 인공신경망 뉴런
- 리그 : 하나가 모든 팀이랑 다 붙음 : 리그 방식 -> 합성곱(내부적으로 곱하기 연산이 들어감)
- 각각의 층은 행렬 : 필터망 
- 역전파 : 입력값 in -> out 
- y (Survive) = wx(각각의 피처) + b  -> 컬럼마다 Survive에 주는 영향(상관 관계)
- bias : x가 0일 때, 아예 값이 없어지니까 최소의 가중치값 1을 줌
- weight는 각각의 피처에 대한 영향력 
- 역전파 : 오답노트 
- 프레임 + 패키지 =>  플랫폼(파이토치, 텐서플로 etc)(배포까지 되면 완성품, 도메인이 존재해야 클라우드에 있으면 플랫폼), 팀장꺼 가져오면, 프레임워크, pnpm install(프레임워크 안에 패키지를 인스톨 함)
- 모델 => AI + 시스템, 모델이 있으면 AI 시스템 / 모델이 없으면 그냥 시스템
- API만 가져오면 시스템 
- ML까지는 모델이 아님. + ai => 모델이됨.
- DS(정형) -> 코퍼스(비정형 텍스트 데이터)
- 코일렉트라(LM) -> Language Model
-  ![[Pasted image 20251215101520.png]]
- 여기서 voca (bow가 강함) -  여기에서만 쓰는 단어 사전. 실제로는 보카가 범위 더큼
- 신경망 (NN)
- 07 - 12
- NNLM(신경망으로 학습된 모델) -> 발전 : RNNLM, BiLM 등 보다 발전된 신경망 언어 모델들
- VLM(비전처리 모델) - 이미지 처리
- feed (먹이를 주다) -> 역전파
- ![[Pasted image 20251215104414.png]]
- ![[Pasted image 20251215104452.png]]
- 원핫 인코딩 모은게 원-핫 벡터
- RNN(LM) - 티라노 : 디폴트 -> 개선 -> `LSTM` `GRU`  -> 덧붙여짐
- 08 - 02(LSTM)
- IT에서 바닐라 -> 무맛 -> 기본 -> 클래식이라고함.
- 텐서플로(플랫폼)
- RNN을 이용하여 텍스트 생성(그전엔 감성 분별만 하다 이제 글씀)
- 08-06
- 에포크 : 훈련 횟수(DS셋 쭉 훈련 끝나는게 1회) 200에포크 200년 
- ex) 초등학교 에포크 6회, 중 3 에포트
- 워드투백(기술)
- ![[Pasted image 20251215110058.png]]
- 하나의 사전을 인덱스 번호를 똑같이 지정. 
- CBOW(Continuous Bag of Words) 안녕뒤에 붙을 수 있는 거 묶어둔거., 연관성있는 단어들의 집합.
- 워드 투백 - 원핫 인코딩이 기본인데, 뒤에 남는 공간이 아까우니까 유사한거 까지 묶어 둔다.
- ![[Pasted image 20251215110546.png]]
- 만들어 진거 결과 값 : 윈도우
- 9-13
- 단어가 끝나고 문서 학습을 시킴 
- 독투백이 가능한 이유, 워드투백 학습이 끝남, 알고리즘을 통해서 학습
- 타이타닉 : DS + 알고리즘(나이브 베이즈 등등) - 피쳐 엔지니어링
- 지금은 알고리즘(워드투백, 독투백)을 통해서 학습을 시킨다.
- 서브워드 알고리즘.
- 번역 알고리즘 X - 어텐션
- 번역 알고리즘 O - transformer
- 파이토치 : 플랫폼, 파이토치 모델 : 파이토치에서 꺼냈다.
- nn(LM)붙으면 모델
- 모델에 여러 알고리즘을 붙일 수 있음(앙상블) 할수는 있는데, 필요한 거.
- 어텐션(메커니즘) : 다른 모델안에 부품 처럼 포함됨.
- 어텐션 : 알고리즘의 구성요소 메커니즘 ⊃ 어텐션
- 알고리즘 내부를 바꿈 : 메커니즘
- 모델 > 알고리즘(0개,1개, 여러개) > 손가락 까닥이는거 메커니즘.
- 어텐션 : 전체 입력 문장을 다시 한 번 참고한다(맥락)
- 메커니즘.
- LSTM 묶은게 NN
- prime(오리지널)
- 알고리즘이 투입되기 전에도 모델 후에도 모델
- `알고리즘 : 분리 가능, 모델은 메커니즘 을 갖는다.`
- BERT 인코더(읽기) 
- GPT 디코더(쓰기)
- 어텐션 : 이전까지는 누군가가 일일이 했는데, 셀프 어텐션 이제 지가 읽고 판단함.
- (Generative(생성형) Pre-trained(미리 트레인시킴) Transformer)
- BART : 트렌스포머 읽기 + 쓰기
- 공통점 : 모델, 어텐션은 둘다 가지고 있음.
- 
- 허깅페이스에서 다운받은 코 일렉트라는 트랜스포머 모델인가?
- 파인튜닝 : 모델을 세밀하게 조정할 수 있다.
- 이모델을 이 데이터를 통해서 훈련시켜줘, 에포크는 5회만 해줘. 많이 하면 OOPT남