- 딥러닝 : CUDA 프로그램 
- 이전까지는 csv의 전처리 였고 이제 부터는 `텍스트 전처리(Text preprocessing)`
- 텍스트란? 워드, text 파일 등등을 말함. 
- 토큰화 - 코퍼스(말뭉치) 데이터 
- Word(단어)일 경우, 단위가 됨.
- https://wikidocs.net/21698
- ![[Pasted image 20251212095726.png]]
- NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공
- KONLPY, JPYE 한국어  
- ![[Pasted image 20251212100030.png]]
- csv : 숫자를 정규화() 하냐, 텍스트 : 문자를 정규화() 하냐 개념은 같지만 방법에서 차이가 있음.
- 정규화 : 푸르딩딩하다, 파랗다 -> 다 같은말로 묶어야함. 
- 작업에 방해가 되는 부분 : 노이즈 
- 토큰 : 단어 문장 문단도 됨
- 어간 추출(Stemming), 표제어 추출(lemmatizaiton)
- Stem과 Lem
- 정규화 = 정해진 틀 안에 모든 데이터를 집어 넣고 줄이는 것. 
- ![[Pasted image 20251212101016.png]]
- 형태소 : 의미를 가진 가장 작은 단위 (아톰)
- stopword ~의/~가 조사 : 불필요한 정보 
- 삼성전자 보고서에서 사용 방법은 stopword 온실가스는 stopword 가 아님.
- ![[Pasted image 20251212102217.png]]
- 정규 표현식 모듈 = re 과 정규화는 다른 개념.
- 특수 문자와 모듈 함수 
-  ![[Pasted image 20251212104116.png]]
- ^학, ?교(학교)
- ![[Pasted image 20251212104208.png]]
- a-z 소문자, ㄱ힣 = re
- ![[Pasted image 20251212104442.png]]
- decimal
- 인코딩 : 숫자로 바꾸는 것
- 원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합(**vocabulary**)을 만드는 일
-  숫자로 바뀐 단어들을 벡터로 다루고 싶다면 어떻게 하면 될까요?
- 백터로 바꿨을 때, 방향성을 가지도록 
- 커다란 종이 전지에 ㄱ ...... 힣 -> 필요한 부분 찢을수 있음.
- ![[Pasted image 20251212105556.png]]
- 2차원에는 방향없음, 가로 세로로 확장성은 없다, 찢어서 1차원으로 갔을때 방향성을 가짐.
- GPU상태에서 하나만 핫하게 만든다.  0000000 원하는 단어 1 단어0 단어 000000
- 한줄에 만개 인코딩,  여러줄로 쪼개서 하나만 오는게 원핫 인코딩.
- ![[Pasted image 20251212110629.png]]
- csv , feature 전처리
- 텍스트 전처리
- 언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델
- 빈도수(BagofOword bow) 별로 주는거. 삼성전자하면 삼성전자
- Word2Vec 단어를 백터로 만든다.
- 
- @ai.kanggyeonggu.store/mlservice/app/nlp/data/kr-Report_2018.txt 여기에서, 한국어 Bow를 만드는데 사용할 국어사전 라이브러리를 다운받고 싶은데, 어느 것을 추천하고 한번 설정해서 지속적으로 사용할 수 있는 전략을 알려줘.
- 
- ![[Pasted image 20251212114416.png]]
- 보통 동시에 여러개가 작동하는데 오염된 데이터가 섞일 수 있어서, 
- `with` 블록에 들어가면 **리소스를 열고**, 블록을 빠져나올 때(정상 종료든 에러든 상관없이) **자동으로 정리해주는 구조**야.
- 
- 원하는 이미지를 넣는 것 - 마스크
- 
- 스팸메일 판단, 감정 분석
- ![[Pasted image 20251212131148.png]]
- NLP에서는 코퍼스, bow, 편집하는거 토큰 -> 이걸 하는 행동을 
- csv, DS DF -> 이걸 하는 회사는 Feature Engenieer 
- 한국어 사전(voca) 일부 추출해서 bow(단어의 빈도수), 
- 
- 비전처리 이미지 처리

- https://x.com/lab_zang/status/1999353634203836672
- 훈련시키는거 모델 핏
- 퍼셉트론(perception 지각), 이미테이션 모방(시뮬레이션=벤브)
- 인셉션 - 팽이 돌아가면 꿈인것 처럼 가상으로 돌아가다가 통로를 통해서
- 디스크 실제(현실), 가상화
- ![[Pasted image 20251212150817.png]]
- 값이 왔다가 가는 과정에서 가중치가 붙어서 강화과정이 됨.
- 어떤단어가 별점하고 상관관계를 맺는가.
- 각 피처마다 수동으로 가중치를 매길수 있음.
- 각각의 입력값에는 각각의 가중치가 존재하는데, 이때 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미합니다.
- x(row data)에 있는 피처
- ![[Pasted image 20251212151559.png]]
- 퍼셉트론(클래식)에서 발전한게 인공신경망(ANN) - 퍼셉트론 여러개
- ![[Pasted image 20251212151914.png]]
- 퍼셉트론이 층(레이어)이 많아 지니까  더 똑똑해짐 Deep해지니까 deep 러닝
- 이 리뷰가 중요하고 rating이 중요하다는 걸 가져오는 모델이 있음.
- 활성화 함수 : 의미있는 것만 살리고 나머지는 지운다.(피처빼는 거임 개고양이 판별하는데 상관없으니까)
- ![[Pasted image 20251212154632.png]]
- 전체를 합쳐서 65로면 살리자 - 시그모이드 함수
- 시퀀스 - 모델 
- ![[Pasted image 20251212155738.png]]
- 7-3 필터에 망이 댄스(Dense)가 있고 시퀀셜이 있음
- 부드럽게 쪼갠다.
- model.add(Dense(2, input_dim=3, activation='softmax'))
- 레이어 2층이고 , 디멘션 3차원
- y=wx + b (bias 편차)
- 7-4 예측 한다음에 맞춰주는거 손실 함수
- 7-5 역전함수 틀렸어 다시 맞춰 이러고 값 다시 돌려 보내주는거.
- 그러면 쟤네가 가중치를 조절함.
- 7-6
- 7-8 케라스 사용할 예정
- 단어를 원-핫 벡터로 만드는 과정을 원-핫 인코딩이라고 한다면, 단어를 밀집 벡터로 만드는 작업을 **워드 임베딩(word embedding)** 이라고 합니다.
- 원핫 인코딩이 여러개면 워드 임베딩
- ![[Pasted image 20251212170924.png]]
- https://huggingface.co/monologg/koelectra-small-v3-discriminator/tree/main