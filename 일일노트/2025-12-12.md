- 딥러닝 : CUDA 프로그램 
- 이전까지는 csv의 전처리 였고 이제 부터는 `텍스트 전처리(Text preprocessing)`
- 텍스트란? 워드, text 파일 등등을 말함. 
- 토큰화 - 코퍼스(말뭉치) 데이터 
- Word(단어)일 경우, 단위가 됨.
- https://wikidocs.net/21698 위키독 딥러닝
- ![[Pasted image 20251212095726.png]]
- NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공
- KONLPY, JPYE 한국어  
- ![[Pasted image 20251212100030.png]]
- csv : 숫자를 정규화() 하냐, 텍스트 : 문자를 정규화() 하냐 개념은 같지만 방법에서 차이가 있음.
- 정규화 : 푸르딩딩하다, 파랗다 -> 다 같은말로 묶어야함. 
- 작업에 방해가 되는 부분 : 노이즈 
- 토큰 : 단어 문장 문단도 됨
- 어간 추출(Stemming), 표제어 추출(lemmatizaiton)
- Stem과 Lem
- 정규화 = 정해진 틀 안에 모든 데이터를 집어 넣고 줄이는 것. 
- ![[Pasted image 20251212101016.png]]
- 형태소 : 의미를 가진 가장 작은 단위 (아톰)
- stopword ~의/~가 조사 : 불필요한 정보 
- 삼성전자 보고서에서 사용 방법은 stopword 온실가스는 stopword 가 아님.
- ![[Pasted image 20251212102217.png]]
- 정규 표현식 모듈 = re 과 정규화는 다른 개념.
- 특수 문자와 모듈 함수 
-  ![[Pasted image 20251212104116.png]]
- ^학, ?교(학교)
- ![[Pasted image 20251212104208.png]]
- a-z 소문자, ㄱ힣 = re
- ![[Pasted image 20251212104442.png]]
- decimal
- 인코딩 : 숫자로 바꾸는 것
- 원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합(**vocabulary**)을 만드는 일
-  숫자로 바뀐 단어들을 벡터로 다루고 싶다면 어떻게 하면 될까요?
- 백터로 바꿨을 때, 방향성을 가지도록 
- 커다란 종이 전지에 ㄱ ...... 힣 -> 필요한 부분 찢을수 있음.
- ![[Pasted image 20251212105556.png]]
- 2차원에는 방향없음, 가로 세로로 확장성은 없다, 찢어서 1차원으로 갔을때 방향성을 가짐.
- GPU상태에서 하나만 핫하게 만든다.  0000000 원하는 단어 1 단어0 단어 000000
- 한줄에 만개 인코딩,  여러줄로 쪼개서 하나만 오는게 원핫 인코딩.
- ![[Pasted image 20251212110629.png]]
- csv , feature 전처리
- 텍스트 전처리
- 언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델
- 빈도수(BagofOword bow) 별로 주는거. 삼성전자하면 삼성전자
- Word2Vec 단어를 백터로 만든다.
- 
- @ai.kanggyeonggu.store/mlservice/app/nlp/data/kr-Report_2018.txt 여기에서, 한국어 Bow를 만드는데 사용할 국어사전 라이브러리를 다운받고 싶은데, 어느 것을 추천하고 한번 설정해서 지속적으로 사용할 수 있는 전략을 알려줘.
- 
- ![[Pasted image 20251212114416.png]]
- 보통 동시에 여러개가 작동하는데 오염된 데이터가 섞일 수 있어서, 
- `with` 블록에 들어가면 **리소스를 열고**, 블록을 빠져나올 때(정상 종료든 에러든 상관없이) **자동으로 정리해주는 구조**야.
- 
- 극
- 시퀀스 - 모델 
- ![[Pasted image 20251212155738.png]]
- 7-3 필터에 망이 댄스(Dense)가 있고 시퀀셜이 있음
- 부드럽게 쪼갠다.
- model.add(Dense(2, input_dim=3, activation='softmax'))
- 레이어 2층이고 , 디멘션 3차원
- y=wx + b (bias 편차)
- 7-4 예측 한다음에 맞춰주는거 손실 함수
- 7-5 역전함수 틀렸어 다시 맞춰 이러고 값 다시 돌려 보내주는거.
- 그러면 쟤네가 가중치를 조절함.
- 7-6
- 7-8 케라스 사용할 예정
- 단어를 원-핫 벡터로 만드는 과정을 원-핫 인코딩이라고 한다면, 단어를 밀집 벡터로 만드는 작업을 **워드 임베딩(word embedding)** 이라고 합니다.
- 원핫 인코딩이 여러개면 워드 임베딩
- ![[Pasted image 20251212170924.png]]
- https://huggingface.co/monologg/koelectra-small-v3-discriminator/tree/main