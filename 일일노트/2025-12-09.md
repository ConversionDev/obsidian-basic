- 모델은 정제된 feature space를 기반으로 패턴을 학습합니다. 
- 정제된 데이터 : refine data 
- 파인 튜닝 :  **새로운 데이터**를 조금 더 넣어서 **특정 목적에 맞게 미세 조정하는 과정**
- 전처리가 곧 모델의 성능을 결정합니다
- 모델 : 학습을 통해 만들어진 뇌(두뇌 프로그램)
- 전처리 파이프라인, 피처셋(df),// 머신러닝 모델, 학습(러닝), 평가 
- 이미 존재하는 머신러닝 모델을 사용함.(Logistic, RandomForest, XGBoost)
- -> 우리가 만드는게 아니다.
- 모델을 선택하고 입맛에 맞게 튜닝 : 모델링
- train + test => 오리지날 승선 인원(DS) 대충 7:3
- 우리가 학습시킬 하나의 DS도 7:3 정도로 나누자
- 전체 과정 : 전처리, 모델, 학습, 평가 
- 서비스는 메소드의 집합
- ![[Pasted image 20251209124937.png]]
- 
-  데이터셋에 속성값을 잡아둔다 
-  ds가 여러개라도 최소한 으로 대응 가능, 속성 값이 많아짐
- 전처리 전에 객체 생성
- 모델 : 기계가 스스로 규칙을 발견한다.
- 사람이 직접 발견하는게 아니라 기계가 자동으로 찾아내게 해야함. 
- 현재 상태는 속성만 있음 -> 알고리즘이 기능이됨![[Pasted image 20251209154628.png]]
- 전처리 : 기계를 위한 속성값 준비 + 모델링 => 객체 => 학습 =>평가
- 범주형과 연속형(실수) -> 선형회귀는 연속형만 가능 :  회귀 : 평균값 중앙값으로 모이더라.
- 로지스틱 회귀 : YES\NO 두가지로 범주형.
- 
- 비지도 : 음성, 이미지, 영상 등등으로 넘어가면 2가지 뺌
- `KNN : 렌덤 분포, 유명함. 선형관계를 전제로 하지 않는다.`
- 현재값 기준으로 어느 정도 범위가 있다.  예외는 있겟지만, 내일의 값을 예측 하는데 앞선 선례의 범위내에 있을 것이다.
- 아웃라이어(이상치) : 제거 대상. 마이클조던, 서장훈 등 평균값을 왜곡 시키는 수치.
- 프롬프트 : 전체 데이터 셋에서 상위 2.5 하위2.5를 제거 시켜줘 95%가 가장 이상적. 지나치게 높으면 과적합(Overfitting), 이 DS에서 아웃라이어를 제거해줘.
- 
- 나이브 베이즈(Naive Bayes) 순진한, 베이즈신부(통계 시초) - NB
- https://www.google.com/search?q=%EB%B2%A0%EC%9D%B4%EC%A6%88+%EC%A0%95%EB%A6%AC+%EC%87%BC%EC%B8%A0+%EB%B0%9C%EB%A0%8C%ED%83%80%EC%9D%B8&sca_esv=414ecf1566e10a89&sxsrf=AE3TifNy_JrjJVQz2MvhbMbPvEMXKA0NPg%3A1765264056801&ei=uMo3acrMMN6-vr0Pp6L02AQ&ved=0ahUKEwjKiKmI-a-RAxVen68BHScRHUsQ4dUDCBE&uact=5&oq=%EB%B2%A0%EC%9D%B4%EC%A6%88+%EC%A0%95%EB%A6%AC+%EC%87%BC%EC%B8%A0+%EB%B0%9C%EB%A0%8C%ED%83%80%EC%9D%B8&gs_lp=Egxnd3Mtd2l6LXNlcnAiJOuyoOydtOymiCDsoJXrpqwg7Ie87LigIOuwnOugjO2DgOyduDIFECEYoAFIrhdQhAdYjxZwA3gAkAEBmAGhAaABkwyqAQQwLjEyuAEDyAEA-AEBmAIIoAKVBcICCxAAGIAEGLADGKIEwgIEECMYJ8ICCBAAGIAEGKIEmAMAiAYBkAYFkgcDMy41oAeULLIHAzAuNbgHigXCBwcwLjQuMy4xyAcagAgA&sclient=gws-wiz-serp#fpstate=ive&vld=cid:a8fab990,vid:Y4ecU7NkiEI,st:0 쇼츠
- 
- 결정트리(Decision Tree) = 스무고개
- < - > 
- 렌덤 포레스트 (Random Forest) - 여러개 중에 렌덤 으로 하나
- XGB->LGBM(머신) 라이트 GB머신
- 
- LGBM은 “트리 여러 개를 단계적으로 만들어서, 틀린 부분을 점점 고쳐 나가며 생존 여부를 맞춘다.”
- 수백개 트리의 투표로 생존 예측 - 각각 나무한테 줘서 젤 괜찮은거 뽑는거.
- 

