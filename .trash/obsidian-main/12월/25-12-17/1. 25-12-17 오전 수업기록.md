- RAG는 chain이다 =파이프라인
	- 체인 패턴을 가겠지. 크로마랑 파이쓰는 안 좋아져서 피지백터 사용했고, 리액트를 사용해서 ? 넥스트? 화면을 구성했다
- 랭체인 폴더를 이름을 래그로 바꿈
	- 어제 랭체인에 피지벡터까지 넣은거니까, 화면까지 넣었으니까
- 자바에서는 각 컨서레엔 파일들을 만들면서 개념을 키워나갔고, 
- 지금은 큰 그림인 체인부터 만든 것
- 오늘은 이제 세부적인 그림을 그리는거야 = 파인튜닝 = fine-tuning

### RAG와 미세 조정(fine-tuning) 비교
교안 https://www.redhat.com/ko/topics/ai/rag-vs-fine-tuning

- RAG는 껍데기. fine-tuning이 진짜 내용물
- RAG가 껍데기라는 말은 어제 오픈ai 에서 api 갖다가 넣은 것만 포함한 것
	- RAG는 기반 LLM을 수정하지 않고 이 작업을 수행하는 반면, 미세 조정(fine-tuning)은 LLM의 가중치와 매개 변수를 조정해야 합니다.
- LM의 파운데이션 모델은 트렌스포머
- LLM(GPT, 제미나이, 그록, 클로드,알파카 등)의 파운데이션 모델은 알파카의 파운테이션모델은 라마
	- 라마는 메타꺼
- RAG = 체인 = 아키텍쳐
- RAG 아키텓쳐는 해당 데이터를 LLM의 컨텍스트로 처리함

- fine-tuning 미세조정
	- 파인튜닝은 의도를 LLM에 전달하여 모델이 목표에 맞게 결과를 조정할 수 있도록 하는 방법
	- LoRA와 QLoRA 는 매개변수 효율적인 미세조정(PEFT) 기술입니다.
	- 파인튜닝 기술은 페프트 기술을 사용한다. PEFT (Parameter-Efficient Fine-Tuning)
	- 페프트 중에서 로라와 큐로라를 언제 언제 사용했다
	- 파인튜닝은 이미지 조정에서도 사용함
	![[Pasted image 20251217104642.png]]
	교안 2) https://www.databricks.com/kr/glossary/fine-tuning
	- 어뎁터는 파인튜닝의 핵심이다
		- 어댑터 기반 파인튜닝은 매개변수의 수를 적게 만들어 작동합니다. 어뎁터 - 디자인패턴. RoRA 의 A 는 어뎁터인 어뎁테이션을 뜻함
		- 어뎁터를 제공해서 모델의 전체 아키텍처를 건들이지 않아도 되고 특정 부분만 어뎁터로 특정 부분을 건들여서 모델에게 특정한 목적성을 줄 수 있는것.
		- 큐로라는 양자화 된 것 
	- 파인튜닝을 하기 위한 기술 = 페프트 
	  
	- 가장 중요한 차이 한 줄 요약 :**LoRA는 “학습 파라미터를 줄이는 기법”이고,  QLoRA는 “모델 자체를 극단적으로 가볍게 만든 상태에서 LoRA를 학습하는 기법”이다.**
		- 실무 기준으로 보면
			- LoRA가 적합한 경우
			- GPU 메모리 여유 있음 (A100, H100 등)
			- 추론 속도 중요
			- 양자화로 인한 미세한 성능 손실도 피하고 싶을 때
		-  QLoRA가 적합한 경우
			- **12GB~24GB GPU**
			- 개인/연구 환경
			- 대형 모델(13B, 65B)을 fine-tuning 해야 할 때
			- 클라우드 비용 최소화
	    - 비유로 정리하면
			- **LoRA**  → “몸집은 그대로 두고, 근육 일부만 운동”
			- **QLoRA**  → “몸을 최대한 압축해 놓고, 근육 일부만 운동”

## 버전 찾기
- RTX 3050에서 코알파카를 페프트로 학습하려고 해. 이 경우 로라와 큐로라 중 어느것을 사용하는 게 좋을까?
	- 로라와 큐로라 둘 다 ... 강의실 랩탑에선 불가능해서. 
- 네이버 리뷰 20만건으로 학습한 후 , 평점을 준 뒤 이에 대한 리뷰를 AI 가 작성하는 프로젝트를 시행하려고 해 훈련 기간은 1달 정도 생각해. 그리고 훈련 후의 성과보다는 진행과정에서 부하가 걸리지 않기를 바래. 이때 , 위의 맥락에서 어느 모델이 가장 합당할까 ?

--> 결론 https://huggingface.co/K-intelligence/Midm-2.0-Mini-Instruct/tree/main 에서 다운로드 함. 미듬

- 전체 중 일부 모델이 미듬. 나머지 부분은 건들지 않겠다. 건들지 않는 부분은 freez 한다고 함
- 행렬의 차원 = 파라미터
- ??? 양자화란 언어모델의 매개변수를 실수형에서 정수형으로 바꿈. 파라미터를 줄인다
- 파라미터 = 컬럼 = 피처 = 변수 / 이것을 숫자로 말하기는 랭크
- 큐로라가 양자화 된 버전, 
- ??? 환경을 똑같이 만든다??


- @requirements.txt @Dockerfile 이거 두개는 @backend 여기에 넣어주고, @app.py @api_server.py 이 두개 파일은 fast api 에 해당하는 건데, @backend 여기에 넣어야할지@app 여기에 넣어야할지 잘 모르겠으니 너가 판단해서 적절한 곳에 넣어줘
- @backend/app/__main__.py @backend/api_server.py @backend/app.py 직접 로컬에서 실행하지 않고 도커로 실행할거니까 이 파일중에서 불필요한 파일은 정리하고 필요한거는 @app 폴더 안으로 옮겨줘
- @app 여기에 이 LLM 모델을 주입하는 최적의 폴더 구조를 생성해줘. 모델 주입은 내가 직접 할거니까 너는 아키텍처만 구성해줘.
backend/
  app/
    core/
      __init__.py
      config.py          # 환경/모델 설정 로딩
      logger.py          # 공통 로깅 설정
      errors.py          # 커스텀 예외 정의
    models/
      __init__.py
      base.py            # LLM/Embedding 공통 인터페이스(Protocol/ABC)
      openai.py          # OpenAI 모델 어댑터
      anthropic.py       # Anthropic 등 확장 어댑터 (필요시)
      fake.py            # 로컬/테스트용 더미 모델
    retriever/
      __init__.py
      vector_store.py    # PGVector 초기화/검색 래퍼
      indexer.py         # 문서 적재/인덱싱 책임 분리
    pipelines/
      __init__.py
      rag.py             # RAG 체인 조립(LMM + retriever + prompt)
      prompts.py         # Prompt 템플릿 관리
    api/
      __init__.py
      deps.py            # FastAPI Depends로 주입할 팩토리/싱글톤
      routers/
        __init__.py
        chat.py          # /api/chat 엔드포인트
        health.py        # /health
    main.py              # FastAPI 생성 및 라우터 바인딩
    __init__.py

- 주입 포인트 예시(구현은 직접):
- models/base.py: LLMClient/EmbeddingClient Protocol 정의 → 구체 구현을 models/openai.py 등에서 작성.
- core/config.py: .env 혹은 docker env에서 모델 타입/키/옵션 로드 → 팩토리에서 선택.
- api/deps.py: FastAPI Depends로 get_llm_client(), get_embedding_client(), get_retriever() 등을 제공 → 라우터가 의존성 주입으로 사용.
- pipelines/rag.py: 주입된 LLM/Embedding/Retriever로 체인 조립만 담당(비즈니스 로직 분리).
- retriever/vector_store.py: PGVector 초기화/연결과 검색 래핑 → 모델 교체와 무관하게 재사용.
이 구조로 모델 구현 파일만 추가/대체하면 주입이 용이하고, API는 인터페이스에만 의존하게 됩니다. 더 세분화나 간소화가 필요하면 말씀주세요.