- 파이토치를 사용하는 순간 우리는 ‘머신러닝’이 아니라 ‘딥러닝(신경망 기반 학습)’을 하고 있는 것이다.

#### 7. 딥러닝 개요

1. 퍼셉트론
	- 퍼셉트론은 가장 단순한 신경망
	- 입력 → 가중치 → 출력 (단층으로 아톰에 해당)
		![[Pasted image 20251216090258.png]]
2. 인공 신경망
	- 퍼셉트론의 차원을 높이고 여러 개 연결 = 인공신경망 (A**NN**, Neural Network)

3. 행렬곱으로 이해하는 신경망
	- 곱의 종류인 합성곱과 행렬곱은 토너먼트와 리그 두 가지로 구분 가능함
		- 리그 방식
			- 모든 팀이 모든 팀과 경기
			- 모든 입력이 모든 출력에 영향을 줌
			- 완전연결층 (Fully Connected Layer)
			- 행렬곱 (Matrix Multiplication)
		- 토너먼트 방식
			- 일부만 연결됨
			- 국소적으로만 계산
			- 합성곱(Convolution)
		- 합성곱(CNN)
			→ 이미지처럼 **국소 패턴**을 볼 때 사용
		- 행렬곱(Dense, FC)
		    → 텍스트·수치 데이터처럼 **전체 관계**를 볼 때 사용
		-  NLP 모델(koELECTRA) 는 거의 전부 행렬곱 기반

- [  ]  [  ]  와 [ [  ] ] 차이
	- `[ ]` : 벡터 (1차원)
	- `[ [ ] ]` : 행렬 (2차원)
		![[Pasted image 20251216091010.png]]

- bias  란?
	- 가중치(w)만 있으면  
	    → 입력이 0이면 출력도 항상 0
	    → 기준점을 살짝 이동시켜주는 값
	- 수업내용에서는 '이게 0일 수 있는데, 식에서 b 까지 없어버리면 삭제되는 거니까 임의적으로 편차 b를 주는 것' 라고 함
		![[Pasted image 20251216091108.png]]
	- **x** : 입력 피처(컬럼)
	- **y** : 결과값 (예: survived)
	- **w** : 가중치 (영향력, TITANIC에서 컬럼마다 survived에 주는 영향도)
	- **b** : 편차 조정값
	
5. 역전파
	- 예측값이 틀렸을 때 되돌아가서 수정하는 것. 가충치를 변경 (=오답노트)

- 컬럼 피처 스키마의 차이
	- 자바, 파이썬에서 변수 = DB에서 컬럼 = DF에서 피처
	- 여러개의 컬럼이 하나의 스키마
	- 스키마는 언어 종류가 무엇인지, DF인지 DB인지에 상관없이 공통으로 쓸 수 있는 용어

- 프레임워크 + 패키지 = 플랫폼
	- 플랫폼이 도메인에 배포 된 것 (클라우드에 있음)
	- 프레임워크는 fast api니까 내가 만든 게 아님. 
	- 텐서플로우, 파이토치는 플랫폼
	- ex) 지피티 유로는 플랫폼에 돈 주는 것
	- ex) 불판 버너 프레임워크, 야채 고기 패키지, 지불하는 것은 플랫폼에 대한 것임
	- 프레임워크와 패키지는 오픈되어 있는 것. 그걸로 어떤 플팻폼을 만드냐는 것
- 그렇다면 모델은 무엇인가?


- 팀장 깃허브에서 갖고온 것은 플랫폼이 아님. 클라우드에서 다운 했으면 로컬로 온 것. 프레임워크 pip install / pnpm install 다 해야했던 게, 프레임워크는 가져와졌는데 패키지는 안 들어왔기 때문임
- 패키지는 프레임워크에 설치하는 것. 그러면 모델은 어디다가 설치하는거냐?

- 모델의 유무는 ai를 시스템을 붙인건지 아닌지로 볼 수 있음
- 모델이 없어서? open api를 시스템에 붙이는 것은 ai시스템이 아님. 챗지피티에서 open api 를 갖고오는 것도 모델을 갖고온 것은 아니기 때문에 ai시스템은 아님
- ml서비스까지는 AI는 아님. 머신러닝정도인거지

- ==NN : Neural Network = 인공신경망==
- 보케뷰러리 = 전체 국어 / 특정 프로젝트에서 쓰는 것 = BOW
- koELECTRA  다운 받은 항목에서 보케뷰러리라고 하는 것은 사실 BOW인 것
- LM: Language Model
- NNLM 신경망 언어모델
- 피드포워드 = 역전파같이 왔다 간 것
- RNNLM: 순환신경망 기반 LM
- Transformer LM : 현재 주류

- 밀집벡터
		![[Pasted image 20251215104407.png]]
- 원핫인코딩 모아둔 게 원핫 벡터
- 앞에 단어들만 모아둔 게 BOW
- LM 은 생략하고 부를거라 NN으로 부를거임
	- VM은 비전(이미지)처리 하는 것. LM 끝나면 배울 것

#### 8. RNNLM = RNN = 순환신경망 = Recurrent Neural Network
- RNN 을 개선한 것 LSTM, GRU 나옴 (텐서플로 기반)
- 덧붙여서 파이토치에서도 쓰는거고, 앞으로도 계속 쓸거임

- 바닐라는 기본이라는 뜻을 가짐
- 에포크 = 훈련횟수
	- 초등학교 = 6에포크 / 책을 새로 받아서 공부를 하는 것 = 에포크 1
	- 

2. 장단기 메모리(Long Short-Term Memory, LSTM)
	- 앞서 배운 RNN을 가장 단순한 형태의 RNN이라고 하여 바닐라 RNN(Vanilla RNN)이라고 합니다. (케라스에서는 SimpleRNN) 바닐라 RNN 이후 바닐라 RNN의 한계를 극복하기 위한 다양한 RNN의 변형이 나왔습니다. 이번에 배우게 될 LSTM도 그 중 하나입니다. 앞으로의 설명에서 LSTM과 비교하여 RNN을 언급하는 것은 전부 바닐라 RNN을 말합니다.
	- 장기의존성 문제가 있어서 GRU가 나옴
3. GRU 게이트 순환 유닛(Gated Recurrent Unit, GRU)
	- 보고서 만든다 = 생성형  AI = generate AI
4. X
5. RNN을 이용한 텍스트 생성(Text Generation using RNN)
	- 200 에포크만 해도 충분

#### 9. 워드 임베딩

- 워드를 잘라서 번호로 주면 한 줄로 만들어 = 단어를 벡터로 바꾸는 것
- 원핫인코딩 공간차지가 너무 심하니까 유사한 단어는 번호 하나 밑에 묶어둠(1과 0.5처럼)
- 인덱스 번호를 맞추는 게 워드 투 백 (Word2Vec)
- CBOW(Continuous Bag of Words) : 안녕 뒤에 붙는 하세요 해 한다 등을 붙이는 것
- W = 윈도우, 그 안에 들어가는 게 뭐야?

- 우리가 다운받은 모델은 Word2Vec을 훈련시킨 것임

- Doc2Vec으로 공시 사업보고서 유사도 계산하기
- Word2Vec, Doc2Vec 은 알고리즘이다
- 알고리즘을 통해서 학습한다
- Doc2Vec을 할 수 있다는 건 Word2Vec을 끝냈다는 것
- BPE(Byte Pair Encoding) Doc2Vec, Word2Vec을 쓰는 거?


15. 어텐션
	- 머신러닝에서 어텐션이 무엇인가?
		- 메커니즘이다. 중요한 것에만 집중해서 보는 메커니즘
	- 딥러닝에서 어텐션은 메커니즘인데, 이것이 알고리즘과 같은 의미인가?
		- 다르다.
		- 셀프어텐션: 기존에는 사람이 중요도를 판단했는데, 이제는 기계 스스로 판단

- 트랜스포머는 모델이다
- 트랜스포머는 기존의 메커니즘을 바꿔서 하는 것
	- 순서대로 보지말고 한번에 보라고 하는 것


- 읽기 인코더 번역(쓰는) 디코더
- 지피티 디코더, 구글 버트 인코더(읽기)
- 바트와 버트는 모델이다. 알고리즘이 아님

- 구글 버트 망하고 제미나이 나옴. 모델 철학의 차이가 있었기 때문
- 버트는 옳고그름 판단을 잘했지만, 생성형(글쓰기)를 못 했기 때문에
- 지피티는 보고서 쓸 때, 등급메길 때는 버트 써야 함
- 최소한의 훈련 된 상태 GPT = Generative Pre-trained Transformer
	- 트렌스포머는 글쓰기 용이다


- 깡통은 모든 모델은 트렌스포머이고, 한국어로 유독 잘하게 한거는 코일렉트라