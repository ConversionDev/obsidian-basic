- GPT) 머신러닝과 딥러닝의 차이점
	- 머신러닝: 사람이 특징(feature)을 직접 설계해야 하는 경우가 많음 / CPU
	- 딥러닝: 머신러닝의 하위 개념. 인공신경망(Neural Network)을 깊게 쌓은 모델을 의미. 특징을 사람이 만들 필요 없이, 모델이 스스로 특징을 학습함. / =GPU = 쿠다프로그램
	- 딥러닝은 학습시간이 길고 매우 많은 데이터가 필요하며 디버깅이 거의 불가능
	- 디버깅이 불가능하다는 것은 오류잡기 어렵다는 게 아니라, 클라이언트가 뭘 고쳐달라고 했을 때 고칠 수 없는 상황을 말함

#### 표제어 추출 실습이 최종 목표
#### 자연어처리는 기본적으로 영어가 기본이라 한국어로 하는 게 어려움
1. 은 어제 수업기록에서 자연어처리 준비하기 파트임
2. 텍스트 전처리 (타이타닉 전처리와는 다른 것)
	- 공백, 문장부호, 단어, 형태소, 문장 등으로 토큰화의 기준을 세울 수 있음
		- ==코퍼스란 피쳐엔지니어링에서의 DS 같은 것.==
		- ==피쳐엔지니어링에서의 DF== 는 ==텍스트엔지니어링에서 토큰이나 스탑워드== 같은 것
	- 한국어에서는 구두점이 의미를 갖지만, 영어는 그렇지 않기도 함
	- nltk 는 영어 코퍼스를 토큰화 하기 위한 것 (라이브러리)
	- KoNLPy (코엔엘파이): 한국어 자연어처리 라이브러리 묶음. 내부에 Okt, Mecab, Komoran 등 포함
	- “~자이파이 두 개” ❓

	- ==정제=cleaning== : (의미 해석을 방해하거나 모델 성능을 떨어뜨리는 불필요한 문자/단어 ex. 의미없는 특수문자, 광고문구, 중복문자 등등)
	- ==정규화=normalization== : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다("서울", "서울시", "Seoul", "seoul", "SEOUL"  -> 모두 같은 의미(서울)를 나타내지만 형태가 다름 . 머신러닝 입장에서는 완전히 다른 단어 5개_로 보임. 그래서 이걸 하나의 통일된 형태로 바꿔주는 걸 “문자 정규화”라고 해.)

	- ==stem==(어간) 추출: 단어를 대충 잘라서 뿌리만 남기는 작업, 단어의 핵심 부분, 하지만 **사전에 없는 형태일 수 있음**(먹었다의 먹, 먹는의 먹, 먹고의 먹)
		- stem=True  --> 로 해두면 알아서 분리 됨됨
	- ==lem==(표제어) 추출: 사전에 있는 원래 단어로 되돌리는 작업, **단어의 사전적 기본형**(표준형), 먹었다의 먹다, 먹는의 먹다, 먹고의 먹다 / 하나의 표제어(또는 하나의 토큰) 로 처리하는 것이 목표
		- ex) 분석 목적에 따라
				- “삼성/전자” (분리)        
			    - “삼성전자” (단일 표제어)

	- 형태소: 의미를 가진 가장 작은 단위 = 원자 = 아톰 = 원소의 소와 형태소의 소가 같은 것
	- 형태소의 종류로 어간과 어미가 있음
	- 형태학적 파싱은 
	- fox는 독립적인 형태소다

	- stopword: 조사도 불용어. 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.

	- 정규표현식 라이브러리/모듈 이름 re, Regular Expression
			![[Pasted image 20251212103912.png]]
	- null 허용 = 0개 허용
	- 그래서 + 는 null 허용 없는거지


	- 패딩" 제일 길이가 긴 걸 기준으로 더 짧은 것들은 0으로 
	- 원 핫 인코딩: 인코딩이란 자연어를 숫자로 처리하는 것. 원 핫 인코딩을 위해서는 단어 집합이 필요함 (vocabulary)/  집합에 있는 모든 단어들마다 인덱스를 부여. 
		- 3차원 -> 2차원 -> 1차원. 1차원이 되어야 기계가 알 수 있어 
		- 여기에 없는 건 스탑워드 처리 되겠네
		- 2차원에는 방향이 너무 많아서 벡터라고 하기 어려움
		- 인덱스로 처리하려면 거기까지 가야하는데 
		- 원 핫 인코딩은 원하는 단어의 인덱스에 1을 부여하고 그를 제외한 모든 것의 인덱스에 0
		- 그래픽에 있는 것
	- 한국어 전처리 패키지 
		- PyKoSpacing: 띄어쓰기 오류가 일반적으로 많으므로 합쳐버린 다음에 다시 올바르게 띄어쓰기 진행

3. 언어 모델 (LM)
	- CSV 피처 엔지니어링 <-> LM 은 텍스트 엔지니어링 <-> 지오코딩(이건 수정할 수 없으니까 엔지니어링이 아님)
	- 언어모델이란? 
		- 우리만의 언어 모델을 만들어야 함 (ESG 팀에서, 삼정에서 원하는 거는 제로베이스에서 언어모델 하나를 만들어서 디벨롭 하는 걸 좋아할 것 같음)
	- Bag of Words(BOW): 단어의 빈도수를 카운트하여 단어를 수치화 하는 단어 표현 방법
		- BOW 로 벡터를 만들어


----
##### 라이브러리 설치 전략
- @ai.aifixr.site/mlservice/app/nlp/data/kr-Report_2018.txt 여기에서, 한국어 BOW를 만드는데 사용할 국어사전 라이브러리를 다운받고싶은데, 어느것을 추천하고 한번 설정해서 지속적으로 사용할 수 있는 전략을 알려줘. 실행하지는 말고

- ![[Pasted image 20251212114935.png]]
	- 삼성 워드클라우드 파일 입장에서 보고서 원본이 한 단계 위 부모니까 ' ' 로 표시
	- with 절의 역할?
		-   ㄹㄹㄹ
	- \n 은 엔터를 의미 줄 없는 뭐를 의미?
	- ^ ㄱ-ㅣ가-힣  --> 이게 다 한글을 의미. 뒤에 + 붙으면 뒤에 뭐가 붙는다는거??
	- 